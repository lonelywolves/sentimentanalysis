{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3-1. sentiment and emotion analysis_caption.ipynb","private_outputs":true,"provenance":[],"mount_file_id":"1-9SKcHyTld154OzCBAtklLW-YdTMDXLQ","authorship_tag":"ABX9TyPN560Py5RyQa3nWIpc9HCL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"-Pcpb6b_WJ4f"},"source":["!pip install -U textblob\n","!python -m textblob.download_corpora\n","!pip install bs4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uyGwkOg1WbNM"},"source":["import os\n","import pandas as pd\n","import re\n","import string\n","import nltk\n","\n","from bs4 import BeautifulSoup as bs\n","from textblob import TextBlob\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HkREm6itWcg9"},"source":["def load_files():\n","\n","  path = '/content/drive/MyDrive/졸프/youtuberdata/ConservativeCaptionList'\n","\n","  files = []\n","\n","  for r, d, f in os.walk(path):\n","      for file in f:\n","          if '.txt' and 'TO' in str(file):\n","            \n","            files.append(os.path.join(r, file))\n","  \n","  return files"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aFHcqTCAWdhw"},"source":["paths = load_files()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3M75xXSIWl31"},"source":["paths"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eNHDWrd0WmzY"},"source":["raw_caption_list = []\n","clean_caption_list = []\n","polarity_list = []\n","subjectivity_list = []\n","sentiment_list = []\n","emotion_list = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YcgM3JoTWns9"},"source":["for path in paths:\n","  with open(path) as c:\n","    lines = c.readlines()\n","    for line in lines:\n","      raw_caption_list.append(line)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"840gWRLyWpBY"},"source":["len(raw_caption_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"porqzTGlWp4N"},"source":["def preprocessing(review, remove_stopwords = False ): \n","    review_text = bs(review, \"html5lib\").get_text()\t\n","\n","    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n","\n","    words = review_text.lower().split()\n","\n","    if remove_stopwords: \n","        stops = set(stopwords.words(\"english\"))\n","        words = [w for w in words if not w in stops]\n","        clean_review = ' '.join(words)\n","    else:\n","        clean_review = ' '.join(words)\n","\n","    return clean_review"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k_iEi5N3WsoM"},"source":["def cleaning_caption(data):\n","  clean_captions = []\n","  for caption in data:\n","      clean_captions.append(preprocessing(caption,remove_stopwords=True))\n","  return clean_captions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GraRPhHfWx62"},"source":["clean_caption_list = cleaning_caption(raw_caption_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4mU40M24W7e2"},"source":["def sentiment_analysis(clean_caption):\n","  result=TextBlob(clean_caption)\n","  polarity_list.append(result.polarity)\n","  subjectivity_list.append(result.subjectivity)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VReIvpQKW9Ux"},"source":["def process_sentiment():\n","  for sentiment in polarity_list:\n","    if sentiment>= 0.5 and sentiment<=1.0:\n","\t    sentiment_list.append(\"strongPositive\")\n","    elif sentiment>0.0 and sentiment<0.5:\n","      sentiment_list.append(\"positive\")\n","    elif sentiment == 0:\n","      sentiment_list.append(\"neutral\")\n","    elif sentiment <0 and sentiment>= -0.5:\n","      sentiment_list.append(\"negative\")\n","    elif sentiment < -0.5:\n","      sentiment_list.append(\"strongNegative\")\n","      "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ce1WgN_sXAGi"},"source":["emotion분석"]},{"cell_type":"code","metadata":{"id":"HwsJ9GesW-ja"},"source":["emotion_file = pd.read_csv('/content/drive/MyDrive/졸프/NRC-Emotion-Lexicon-clean.csv',encoding = 'utf-8-sig')\n","emotion_file.info"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Os5GTlDwXCam"},"source":["def flat_list(array): \n","    a=[]\n","    for i in array:\n","        if type(i) == type(list()):\n","            a+=(flat_list(i))       \n","        else:\n","            a.append(i)\n","    return a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qYAeEKohXDgl"},"source":["def extract_emotion_list(text,file):\n","\n","  lower_case = text.lower()\n","\n","  cleaned_text = lower_case.translate(str.maketrans('', '', string.punctuation))\n","\n","  tokenized_words = word_tokenize(cleaned_text, \"english\")\n","\n","# Lemmatization - From plural to single + Base form of a word (example better-> good)\n","  lemma_words = []\n","  for word in tokenized_words:\n","      word = WordNetLemmatizer().lemmatize(word)\n","      lemma_words.append(word)\n","\n","  emotion_lists = []\n","\n","  for j in range(len(file['wordlist'])):\n","      word = file['wordlist'][j]\n","\n","      if word in lemma_words:\n","          emotion_lists.append(file['emotions'][j].split(':'))\n","  emotion_lists = flat_list(emotion_lists)\n","  return emotion_lists\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rz5Q_qjtXEnX"},"source":["for i in range(len(clean_caption_list)):\n","  if i%1000 == 0:\n","    print(i)\n","  sentiment_analysis(clean_caption_list[i])\n","  emotion_list.append(extract_emotion_list(clean_caption_list[i],emotion_file))\n","process_sentiment()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1SAtJ3RUXGBy"},"source":["df = pd.DataFrame({'raw_comment':raw_caption_list, 'clean_comment':clean_caption_list,\n","                   'polarity':polarity_list,'subjectivity':subjectivity_list,\n","                   'sentiment':sentiment_list,'emotion_list':emotion_list})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sg_05sWUXHas"},"source":["df.to_csv(\"/content/drive/MyDrive/졸프/감성분석result/Conservative_Result/T0_Conservative_Caption_result.csv\",encoding = 'utf-8-sig')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_UeXgY8jXZTB"},"source":["  df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lsHLzTNbFBtp"},"source":[""],"execution_count":null,"outputs":[]}]}